{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/madeeltariq/api-gateway-rag?scriptVersionId=214630358\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# *Installing the required libraries*# ","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport sys\nimport os\n\ndef install_package(package_name):\n    try:\n        # Check if the package is installed\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"show\", package_name])\n        print(f\"{package_name} is already installed.\")\n    except subprocess.CalledProcessError:\n        # If package is not installed, install it\n        print(f\"Installing {package_name}...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n        print(f\"{package_name} installation completed.\")\n\ndef install_ngrok():\n    # Check if ngrok is already downloaded and extracted\n    if not os.path.exists('ngrok'):\n        print(\"Downloading and installing ngrok...\")\n        subprocess.run(['wget', '-q', '-O', 'ngrok.zip', 'https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip'])\n        subprocess.run(['unzip', '-o', 'ngrok.zip'])\n        print(\"ngrok installed successfully.\")\n    else:\n        print(\"ngrok is already installed.\")\n\n# Installing packages step-by-step\ninstall_package(\"flask\")\ninstall_package(\"transformers\")\ninstall_package(\"pinecone-client\")\ninstall_package(\"python-dotenv\")\ninstall_package(\"flask-cors\")\ninstall_package(\"pyngrok\")\n\n# Install ngrok separately\ninstall_ngrok()\n\nprint(\"Done With All installations >>>>>>>>>>>>>>>>> Move Forward\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports in here","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Code cell for Entering the ngrok API key For Ngrok implementation**","metadata":{}},{"cell_type":"code","source":"import os\nimport getpass  # Import the getpass module to hide input text\n\n# Function to create/update .env file with ngrok API key\ndef set_ngrok_api_key():\n    # Ask user for the ngrok API key without displaying the input\n    ngrok_api_key = getpass.getpass(\"Please enter your ngrok API key: \")\n    \n    # Check if .env file exists\n    env_file = '.env'\n    \n    # If file exists, append the key; if not, create a new file\n    if os.path.exists(env_file):\n        with open(env_file, 'a') as file:\n            file.write(f\"NGROK_AUTHTOKEN={ngrok_api_key}\\n\")\n    else:\n        with open(env_file, 'w') as file:\n            file.write(f\"NGROK_AUTHTOKEN={ngrok_api_key}\\n\")\n    \n    print(f\"ngrok API key added to {env_file}\")\n\n# Function to create/update .env file with Pinecone credentials\ndef set_pinecone_credentials():\n    # Ask user for the Pinecone API key without displaying the input\n    pinecone_api_key = getpass.getpass(\"Please enter your Pinecone API key: \")\n\n    # Ask user for the Pinecone environment, with a default value\n    pinecone_env = input(\"Please enter your Pinecone environment (e.g., us-west1-gcp) [default: us-east-1]: \") or \"us-east-1\"\n    \n    # Check if .env file exists\n    env_file = '.env'\n    \n    # If file exists, append the keys; if not, create a new file\n    if os.path.exists(env_file):\n        with open(env_file, 'a') as file:\n            file.write(f\"PINECONE_API_KEY={pinecone_api_key}\\n\")\n            file.write(f\"PINECONE_ENV={pinecone_env}\\n\")\n    else:\n        with open(env_file, 'w') as file:\n            file.write(f\"PINECONE_API_KEY={pinecone_api_key}\\n\")\n            file.write(f\"PINECONE_ENV={pinecone_env}\\n\")\n    \n    print(f\"Pinecone credentials added to {env_file}\")\n\n# Call the functions to set the API key and credentials\nset_ngrok_api_key()\nset_pinecone_credentials()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom dotenv import load_dotenv  # Import the dotenv package to load .env file\nfrom pinecone import Pinecone\n\n# Function to get vector from Pinecone database based on ID\ndef get_vector_from_pinecone(file_id, index_name=\"vecotr\", namespace_name=\"newCheck\", pinecone_env=\"us-east-1\"):\n    # Load environment variables from .env file\n    load_dotenv()  # This will load the variables from the .env file into the environment\n\n    # Load Pinecone API key from the environment variables\n    pinecone_api_key = os.getenv('PINECONE_API_KEY')\n    if not pinecone_api_key:\n        print(\"Check if ENV variables are set\")\n\n    # Ensure the API key is not None\n    if not pinecone_api_key:\n        # print(\"Error: Pinecone API key is missing from the environment variables.\")\n        return None\n\n    # Initialize Pinecone client\n    try:\n        pc = Pinecone(api_key=pinecone_api_key)\n        print(f\"Pinecone client initialized with environment: {pinecone_env}\")\n    except Exception as e:\n        print(f\"Error initializing Pinecone client: {str(e)}\")\n        return None\n\n    # Initialize the index\n    try:\n        index = pc.Index(index_name)\n    except Exception as e:\n        print(f\"Error initializing Pinecone index: {str(e)}\")\n        return None\n\n    # Query Pinecone to retrieve vector for the specific ID\n    try:\n        print(f\"Retrieving vector for file ID: {file_id} from Pinecone...\")\n\n        # Querying Pinecone index for the specific ID using metadata filtering\n        response = index.query(\n            namespace=namespace_name,\n            filter={\"fileId\": {\"$eq\": file_id}},\n            id=file_id,\n            top_k=1,\n            include_values=True,\n            include_metadata=True\n        )\n\n        # Check if result contains matches\n        if response and response.get(\"matches\"):\n            # Display the first few vectors based on top_k\n            for idx, vector_data in enumerate(response[\"matches\"]):\n                # print(f\"Vector {idx + 1} ID: {vector_data['id']}\")\n                print(f\"Pinecone Index: {index_name}\")\n                print(f\"Pinecone Env: {pinecone_env}\")\n                # print(f\"Metadata (fileId): {vector_data['metadata']['fileId']}\")\n                # print(f\"Metadata (text): {vector_data['metadata']['text'][:100]}...\")  # Showing first 100 chars of text\n                print(f\"Vector Values: {vector_data['values'][:5]}...\")  # Showing first 5 values of the vector\n                print(f\"Size of Vector Values: {len(vector_data['values'])}\")  # Printing the size of the vector values\n            return response[\"matches\"]\n\n        else:\n            print(f\"No vector found for file ID: {file_id}\")\n            return None\n    except Exception as e:\n        print(f\"Error retrieving vector for file ID {file_id}: {str(e)}\")\n        return None\n\n# Example usage of the function\nfile_id = \"676594520c3eb8c3272efa2c\"  # Replace with the file ID you want to query\n\nvector = get_vector_from_pinecone(file_id)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:09:08.856454Z","iopub.execute_input":"2024-12-24T17:09:08.856753Z","iopub.status.idle":"2024-12-24T17:09:10.088782Z","shell.execute_reply.started":"2024-12-24T17:09:08.85673Z","shell.execute_reply":"2024-12-24T17:09:10.088073Z"}},"outputs":[{"name":"stdout","text":"Pinecone client initialized with environment: us-east-1\nRetrieving vector for file ID: 676594520c3eb8c3272efa2c from Pinecone...\nPinecone Index: vecotr\nPinecone Env: us-east-1\nVector Values: [0.029296875, -0.00984191895, -0.0322265625, -0.0369262695, 0.0473632812]...\nSize of Vector Values: 1024\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# To test this one is ","metadata":{}},{"cell_type":"code","source":"pip install pinecone-client requests python-dotenv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:17:31.097905Z","iopub.execute_input":"2024-12-24T17:17:31.098287Z","iopub.status.idle":"2024-12-24T17:17:34.231466Z","shell.execute_reply.started":"2024-12-24T17:17:31.098262Z","shell.execute_reply":"2024-12-24T17:17:34.230219Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (5.0.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\nRequirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\nRequirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.1.0)\nRequirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.0.7)\nRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.5)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\nfrom dotenv import load_dotenv\nfrom pinecone import Pinecone, ServerlessSpec\n\n# Load environment variables (for Pinecone API Key and other configurations)\nload_dotenv()\n\n# Initialize Pinecone client\npc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n\n# Define the Pinecone index name and namespace\nindex_name = \"vecotr\"  # Ensure this matches your actual index name\nnamespace_name = \"newCheck\"  # Ensure this matches your namespace name\n\n# Define the embedding model\nmodel = \"multilingual-e5-large\"  # You can replace with any model of your choice\n\n# Initialize Pinecone Index (Ensure your index exists or create it)\nif index_name not in pc.list_indexes().names():\n    pc.create_index(\n        name=index_name,\n        dimension=1024,  # Use 1024 dimensions as per your embedding model\n        metric='cosine',  # Use cosine similarity\n        spec=ServerlessSpec(cloud='aws', region='us-west-2')\n    )\n\n# Get the index\nindex = pc.index(index_name)\n\n# Function to generate embeddings using Pinecone's Inference API\ndef generate_embeddings(text):\n    try:\n        # Generate embeddings using Pinecone's inference API\n        response = index.query(\n            vector=[text],  # Input text to be embedded\n            top_k=1,  # You can specify how many top matches you want (e.g., 1)\n            namespace=namespace_name,\n            include_values=True,  # Ensure you retrieve the vector values\n            include_metadata=True,  # Include metadata if required\n        )\n        \n        # Extract the vector values (assuming the first match)\n        embeddings = response['matches'][0]['values']\n        \n        # Ensure the embeddings are of size 1024\n        if len(embeddings) != 1024:\n            raise ValueError(f\"Generated embeddings have incorrect size: {len(embeddings)} (expected 1024)\")\n\n        return embeddings\n\n    except Exception as e:\n        print(f\"Error generating embeddings: {e}\")\n        raise ValueError(\"Embedding generation failed.\")\n\n# Example: Using the function to get embeddings for a query\nquery = \"What is the university name that is teaching the compiler construction course?\"\nquery_vector = generate_embeddings(query)\nprint(f\"Generated Vector: {query_vector[:5]}...\")  # Display first 5 values of the vector\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:26:39.367906Z","iopub.execute_input":"2024-12-24T17:26:39.36823Z","iopub.status.idle":"2024-12-24T17:26:39.550006Z","shell.execute_reply.started":"2024-12-24T17:26:39.368204Z","shell.execute_reply":"2024-12-24T17:26:39.548929Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-e2d8ecf049b8>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Get the index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Function to generate embeddings using Pinecone's Inference API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Pinecone' object has no attribute 'index'"],"ename":"AttributeError","evalue":"'Pinecone' object has no attribute 'index'","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nfrom dotenv import load_dotenv\nimport os\nfrom pinecone import Pinecone\n\n# Load the environment variables from .env file\nload_dotenv()\n\n\n\n\n# Function to retrieve and compare query vector with Pinecone vectors using cosine similarity\ndef find_similar_vectors(query):\n    # Initialize the sentence transformer model that matches Pinecone's vector dimensionality (1024 in this case)\n    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # You can change this to a 1024-dimensional model\n    \n    # Vectorize the query\n    query_vector = model.encode([query])[0]  # Convert the query into a vector\n\n    # Fetch vector data from Pinecone for a sample file\n    file_id = \"676594520c3eb8c3272efa2c\"  # Replace with your actual file ID\n    result = get_vector_from_pinecone(file_id)\n\n    if result is None:\n        print(\"No results found.\")\n        return\n\n    # Extract the first match from the result\n    if isinstance(result, list) and len(result) > 0:\n        match = result[0]  # Get the first match from the list of results\n\n        # Extract the vector and metadata from the match\n        stored_vector = match['values']\n        metadata = match['metadata']\n\n        # Check if the query vector and the stored vector have the same dimensionality\n        if len(query_vector) != len(stored_vector):\n            print(f\"Error: Dimensionality mismatch! Query vector length: {len(query_vector)}, Stored vector length: {len(stored_vector)}\")\n            return\n\n        # Calculate cosine similarity between the query vector and the Pinecone vector\n        similarity = cosine_similarity([query_vector], [stored_vector])[0][0]\n\n        # Print the similarity and the metadata of the most similar vector\n        print(f\"Cosine Similarity: {similarity:.4f}\")\n        print(f\"Metadata (Text): {metadata.get('text', 'No text metadata available')}\")\n        print(f\"Stored Vector: {stored_vector[:5]}...\")  # Display first 5 values of the stored vector\n\n        return similarity, metadata\n    else:\n        print(\"No valid matches found in the result.\")\n        return None\n\n# Example usage\nquery = \"What is the university name that is teaching the compiler construction course?\"  # Example query\nsimilarity, metadata = find_similar_vectors(query)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:18:29.238076Z","iopub.execute_input":"2024-12-24T17:18:29.238399Z","iopub.status.idle":"2024-12-24T17:18:29.268009Z","shell.execute_reply.started":"2024-12-24T17:18:29.238374Z","shell.execute_reply":"2024-12-24T17:18:29.266882Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-c696ec63c458>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Initialize Pinecone client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpinecone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PINECONE_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PINECONE_ENV\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Define the Pinecone index name and namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/deprecation_warnings.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n"],"ename":"AttributeError","evalue":"init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\ndef get_response_from_llm(query, model_name=\"EleutherAI/gpt-neo-1.3B\"):\n    \"\"\"\n    This function takes a user query, processes it using a free LLM from Hugging Face, \n    and returns the generated response.\n\n    :param query: User input query.\n    :param model_name: Model name to be used for text generation.\n    :return: LLM's response to the query.\n    \"\"\"\n    # Load the model pipeline\n    generator = pipeline(\"text-generation\", model=model_name, device=-1)  # Use CPU (-1) or GPU (set device to appropriate value)\n\n    # Generate response\n    response = generator(query, max_new_tokens=100, do_sample=True)\n    \n    # Return the generated response\n    return response[0][\"generated_text\"]\n\n# Example usage\nuser_query = \"What is the significance of artificial intelligence?\"\nresponse = get_response_from_llm(user_query)\nprint(\"Response from the LLM:\", response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T16:39:27.708333Z","iopub.execute_input":"2024-12-24T16:39:27.708689Z","iopub.status.idle":"2024-12-24T16:39:53.48549Z","shell.execute_reply.started":"2024-12-24T16:39:27.708658Z","shell.execute_reply":"2024-12-24T16:39:53.484768Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Response from the LLM: What is the significance of artificial intelligence?\n\nArtificial intelligence is taking the place of humans in almost all fields and there is no reason to believe that artificial intelligence will replace us in the near future, if ever. Some people say that artificial intelligence will take over everything after we have achieved a peak and there is also a lot of hype around this topic. However, the truth is that we are not going to take over the world until we have built machines to mimic our intelligence. The way we work today makes it tough to be more\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Implemented Ngrok and CORS on top of the Flask API","metadata":{}},{"cell_type":"code","source":"import os\nimport threading\nimport time\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS  # Import CORS to handle cross-origin requests\nfrom pyngrok import ngrok  # Import ngrok\nfrom dotenv import load_dotenv  # Import the dotenv package to load .env file\nfrom pinecone import Pinecone\n\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Function to get vector from Pinecone database based on ID\ndef get_vector_from_pinecone(file_id, index_name=\"vecotr\", namespace_name=\"newCheck\", pinecone_env=\"us-east-1\"):\n  \n    \n    # print(\"The File id given to the fucntion is :\", file_id)\n    # Load Pinecone API key from the environment variables\n    pinecone_api_key = os.getenv('PINECONE_API_KEY')\n    if not pinecone_api_key:\n        print(\"Check if ENV variables are set\")\n\n    # Ensure the API key is not None\n    if not pinecone_api_key:\n        # print(\"Error: Pinecone API key is missing from the environment variables.\")\n        return None\n\n    # Initialize Pinecone client\n    try:\n        pc = Pinecone(api_key=pinecone_api_key)\n        # print(f\"Pinecone client initialized with environment: {pinecone_env}\")\n    except Exception as e:\n        print(f\"Error initializing Pinecone client: {str(e)}\")\n        return None\n\n    # Initialize the index\n    try:\n        index = pc.Index(index_name)\n    except Exception as e:\n        print(f\"Error initializing Pinecone index: {str(e)}\")\n        return None\n\n    # Query Pinecone to retrieve vector for the specific ID\n    try:\n        print(f\"Retrieving vector for file ID: {file_id} from Pinecone...\")\n\n        # Querying Pinecone index for the specific ID using metadata filtering\n        response = index.query(\n            namespace=namespace_name,\n            filter={\"fileId\": {\"$eq\": file_id}},\n            id=file_id,\n            top_k=1,\n            include_values=True,\n            include_metadata=True\n        )\n\n        # Check if result contains matches\n        if response:\n            return response\n\n        else:\n            print(f\"No vector found for file ID: {file_id}\")\n            return None\n    except Exception as e:\n        print(f\"Error retrieving vector for file ID {file_id}: {str(e)}\")\n        return None\n\n\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Enable CORS for all routes\nCORS(app)\n\n# Health check endpoint with request body and logging\n@app.route('/get_model_response', methods=['POST'])\ndef get_model_response():\n    print(\"[DEBUG] Received request at /get_model_response\")\n    try:\n        # Log the received request data\n        data = request.get_json()\n        print(f\"[DEBUG] Request body: {data}\")\n\n        # Validate the request body\n        if not data:\n            print(\"[ERROR] No JSON data received in the request.\")\n            return jsonify({\"error\": \"Request body must be JSON\"}), 400\n\n        file_id = data.get(\"file_id\")\n        if not file_id:\n            print(\"[ERROR] 'file_id' is missing in the request body.\")\n            return jsonify({\"error\": \"'file_id' is required\"}), 400\n\n        # Call the function to get vector from Pinecone\n        vector_response = get_vector_from_pinecone(file_id)\n        if vector_response and vector_response.get(\"matches\"):\n            matches = vector_response[\"matches\"]\n            if matches:\n                # Display the first few vectors based on top_k\n                for idx, vector_data in enumerate(matches):\n                    print(f\"Vector Values: {vector_data['values'][:5]}...\")\n                    print(f\"Size of Vector Values: {len(vector_data['values'])}\")\n                \n                # Only send back the vector_data part\n                # print(\"the vector data is:\", vector_data['values'])\n                return jsonify({\"status\": \"success\", \"data\": vector_data['values']}), 200\n\n        else:\n            print(\"[WARNING] No vector found for the provided file_id.\")\n            return jsonify({\"status\": \"error\", \"message\": \"No vector found for the given file_id\"}), 404\n\n    except Exception as e:\n        print(f\"[ERROR] Exception occurred: {str(e)}\")\n        return jsonify({\"status\": \"error\", \"message\": \"An error occurred\"}), 500\n\n\n@app.route('/test', methods=['GET'])\ndef test():\n    return jsonify({\"message\": \"Flask server is running!\"})\n\n# Function to run Flask server\ndef run_flask():\n    # print(\"Flask server is listening on port 5000...\")\n    app.run(host=\"0.0.0.0\", port=5000)  # Running on port 5000\n\n# Function to start ngrok and expose the Flask server\ndef run_ngrok():\n    # Authenticate ngrok using the API key from the .env file\n    ngrok_auth_token = os.getenv(\"NGROK_AUTHTOKEN\")\n    if ngrok_auth_token:\n        ngrok.set_auth_token(ngrok_auth_token)\n        # print(\"Ngrok authenticated successfully.\")\n    else:\n        print(\"Ngrok authentication failed. Please check your API key.\")\n        return None\n\n    # Open a ngrok tunnel to the Flask app on port 5000\n    public_url = ngrok.connect(5000)\n    print(f\"Ngrok public URL: {public_url}\")\n    return public_url\n\n# Start Flask server in a separate thread to keep the notebook running\nflask_thread = threading.Thread(target=run_flask)\nflask_thread.daemon = True  # Allow thread to terminate when the program exits\nflask_thread.start()\n\n# Start ngrok and print the URL where Flask is accessible\npublic_url = run_ngrok()\n\n# Keep the notebook running to maintain the server\nwhile True:\n    time.sleep(1)  # Just keeps the notebook running\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *Working Prototype for flask listening***","metadata":{}},{"cell_type":"code","source":"import os\nfrom flask import Flask, request, jsonify\nimport threading\nimport time\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Health check endpoint\n@app.route('/health', methods=['GET'])\ndef health_check():\n    return jsonify({\"status\": \"Flask server is running Greate!\"})\n\n@app.route('/test', methods=['GET'])\ndef test():\n    return jsonify({\"message\": \"Flask server is running!\"})\n\n# Function to run Flask server\ndef run_flask():\n    print(\"Flask server is listening on port 8000...\")\n    app.run(host=\"0.0.0.0\", port=9000)  # Running on port 8000\n\n# Start Flask server in a separate thread to keep the notebook running\nflask_thread = threading.Thread(target=run_flask)\nflask_thread.daemon = True  # Allow thread to terminate when the program exits\nflask_thread.start()\n\n# Print out the URL where Flask is running\nprint(\"Flask server should be accessible at: http://0.0.0.0:8000\")\n\n# Keep the notebook running to maintain the server\nwhile True:\n    time.sleep(1)  # Just keeps the notebook running\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# How to send request from the JavaScript Server Side ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}